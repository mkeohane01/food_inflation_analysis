{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '../data/News_Dataset_RAW/' created.\n"
     ]
    }
   ],
   "source": [
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from transformers import TextClassificationPipeline\n",
    "import torch        \n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# sections_scrape = ['better-business', 'business-to-business', 'food', 'global-development', 'lifeandstyle', 'money', 'news', 'politics', 'society', 'world']\n",
    "sections_scrape = ['business','money']\n",
    "dataset_loc = '../data/News_Dataset_RAW/'\n",
    "dataset_procesed_loc = '../data/News_Dataset_preprocessed/'\n",
    "\n",
    "# Check if the raw dataset folder exists, and create it if not\n",
    "if not os.path.exists(dataset_loc):\n",
    "    os.makedirs(dataset_loc)\n",
    "    print(f\"Directory '{dataset_loc}' created.\")\n",
    "\n",
    "# Check if the processed dataset folder exists, and create it if not\n",
    "if not os.path.exists(dataset_procesed_loc):\n",
    "    os.makedirs(dataset_procesed_loc)\n",
    "    print(f\"Directory '{dataset_procesed_loc}' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_pages(url,params):\n",
    "    response = requests.get(url, params=params)\n",
    "    data = json.loads(response.text)\n",
    "    return data['response']['pages']\n",
    "\n",
    "\n",
    "def get_data(pages_to_scrape,section):\n",
    "        all_data = []\n",
    "\n",
    "        last_page_id = ''\n",
    "        for page in tqdm(range(1, pages_to_scrape + 1)):\n",
    "\n",
    "            if page==1:\n",
    "                url = \"https://content.guardianapis.com/search\"\n",
    "                params = {\n",
    "                    'show-tags': 'all',\n",
    "                    # 'show-fields': 'production-office,lang,sectionId,sectionName,webTitle,webUrl,headline,trailText,bodyText,firstPublicationDate,productionOffice',\n",
    "                    'show-fields': 'all',\n",
    "                    # 'page': 1,\n",
    "                    'page-size': 200,\n",
    "                    'api-key': 'test',\n",
    "                    'from-date': '1990-01-01',\n",
    "                    'to-date': '2023-01-20',\n",
    "                    'show-references': 'all',\n",
    "                    'use-date':'published',\n",
    "                    'section': str(section)\n",
    "                }\n",
    "            else:\n",
    "                url = f\"https://content.guardianapis.com/content/{last_page_id}/next?\"\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                articles = data['response']['results']\n",
    "                last_page_id = data['response']['results'][-1]['id']\n",
    "                all_data.extend(articles)\n",
    "            else:\n",
    "                print(f\"Error on page {page}: {response.status_code} - {response.text}\")\n",
    "\n",
    "        return all_data\n",
    "\n",
    "\n",
    "def download_data(dataset_loc,sections_scrape):\n",
    "\n",
    "    \n",
    "\n",
    "    for section in sections_scrape:\n",
    "\n",
    "        if os.path.isfile(f'{dataset_loc}\\\\dataset_{str(section)}.json'):\n",
    "            print(f'Raw Datset for section: {section} Exists.')\n",
    "            continue\n",
    "        \n",
    "        url = \"https://content.guardianapis.com/search\"\n",
    "        params = {\n",
    "            'show-tags': 'all',\n",
    "            # 'show-fields': 'production-office,lang,sectionId,sectionName,webTitle,webUrl,headline,trailText,bodyText,firstPublicationDate,productionOffice',\n",
    "            'show-fields': 'all',\n",
    "            'page': 1,\n",
    "            'page-size': 200,\n",
    "            'api-key': 'test',\n",
    "            'from-date': '1990-01-01',\n",
    "            'to-date': '2023-01-20',\n",
    "            'show-references': 'all',\n",
    "            'use-date':'published',\n",
    "            'section': str(section)\n",
    "        }\n",
    "\n",
    "\n",
    "        total_pages = get_total_pages(url,params)\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "        # Specify the number of pages you want to scrape\n",
    "        pages_to_scrape = total_pages\n",
    "\n",
    "        result_data = get_data(pages_to_scrape,section)\n",
    "        print(f\"Total articles retrieved: {len(result_data)}\")\n",
    "\n",
    "        dataset = {}\n",
    "\n",
    "        dataset['results'] = result_data\n",
    "\n",
    "        # Save the dictionary to a JSON file\n",
    "        with open(f'{dataset_loc}\\\\dataset_{str(section)}.json', 'w') as json_file:\n",
    "            json.dump(dataset, json_file)\n",
    "\n",
    "\n",
    "\n",
    "def make_df(dataset_loc,sections_scrape,dataset_procesed_loc,calculate_sentiment=True):\n",
    "\n",
    "    \n",
    "    for section in sections_scrape:\n",
    "        \n",
    "        print(\"*\"*100)\n",
    "        print(f'Working on {section}')\n",
    "        print(\"*\"*100)\n",
    "        # Assuming your JSON data is stored in a file named 'data.json'\n",
    "        with open(f'{dataset_loc}\\\\dataset_{str(section)}.json', 'r') as file:\n",
    "            result_data = json.load(file)\n",
    "        \n",
    "        small_data = result_data['results']\n",
    "\n",
    "        titles = [data['fields']['headline'] for data in small_data]\n",
    "        body_text = [data['fields']['bodyText'] for data in small_data]\n",
    "\n",
    "\n",
    "        publication_date = []\n",
    "        count= 0\n",
    "        for data in small_data:\n",
    "            if 'firstPublicationDate' in data['fields']:\n",
    "                publication_date.append(data['fields']['firstPublicationDate'])\n",
    "                count += 1\n",
    "            elif 'newspaperEditionDate' in data['fields']:\n",
    "                publication_date.append(data['fields']['newspaperEditionDate'])\n",
    "                count += 1\n",
    "            elif 'webPublicationDate' in data:\n",
    "                publication_date.append(data['webPublicationDate'])\n",
    "                count += 1\n",
    "            else:\n",
    "                print(json.dumps(data))\n",
    "                publication_date.append(None)  # or any default value if both keys are missing\n",
    "                break\n",
    "\n",
    "\n",
    "        production_office = []\n",
    "\n",
    "        for data in small_data:\n",
    "            if 'productionOffice' in data['fields']:\n",
    "                production_office.append(data['fields']['productionOffice'])\n",
    "            else:\n",
    "                production_office.append(None)  # or any default value if both keys are missing\n",
    "\n",
    "\n",
    "        lang = [data['fields']['lang'] for data in small_data]\n",
    "\n",
    "\n",
    "        article_tags = []\n",
    "        for articles in small_data:\n",
    "            # for article_data in articles['tags']:\n",
    "            #     # print([tags_data['id'] for tags_data in articles['tags']])\n",
    "            #     # article_tags.append([tags_data['id'] for tags_data in articles['tags']])\n",
    "            article_tags.append([tags_data['id'] for tags_data in articles['tags']])\n",
    "\n",
    "\n",
    "        # Get length of each variable\n",
    "        length_titles = len(titles)\n",
    "        length_body_text = len(body_text)\n",
    "        length_publication_date = len(publication_date)\n",
    "        length_production_office = len(production_office)\n",
    "        length_lang = len(lang)\n",
    "        length_article_tags = len(article_tags)\n",
    "\n",
    "        # Print the lengths\n",
    "        print(f'Length of titles: {len(small_data)}')\n",
    "        print(f'Length of titles: {length_titles}')\n",
    "        print(f'Length of body_text: {length_body_text}')\n",
    "        print(f'Length of publication_date: {length_publication_date}')\n",
    "        print(f'Length of production_office: {length_production_office}')\n",
    "        print(f'Length of lang: {length_lang}')\n",
    "        print(f'Length of article_tags: {length_article_tags}')\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Title': titles,\n",
    "            'BodyText': body_text,\n",
    "            'PublicationDate': publication_date,\n",
    "            'ProductionOffice': production_office,\n",
    "            'Lang': lang,\n",
    "            'ArticleTags': article_tags\n",
    "        })\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_file_path = f'{dataset_procesed_loc}\\\\dataset_{section}.xlsx'\n",
    "        df = df.applymap(lambda x: x.encode('unicode_escape').\n",
    "                         decode('utf-8') if isinstance(x, str) else x)\n",
    "        \n",
    "        df['PublicationDate'] = df['PublicationDate'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%dT%H:%M:%SZ'))\n",
    "        df['PublicationDate_dates'] = df['PublicationDate'].dt.date\n",
    "\n",
    "        df['PublicationDate_dates'] = pd.to_datetime(df['PublicationDate_dates'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "        df.to_excel(f'{dataset_procesed_loc}\\\\dataset_{section}_WIHTOUT_Sentiments.xlsx', index=False)\n",
    "        \n",
    "        df = get_sentiments(df,section,dataset_procesed_loc,calculate_sentiment)\n",
    "\n",
    "        # get_word_cloud(section,df)\n",
    "\n",
    "        # df.to_excel(excel_file_path, index=False)\n",
    " \n",
    "\n",
    "def get_sentiments(df,section,dataset_procesed_loc,calculate_sentiment=True):\n",
    "\n",
    "    \n",
    "    if calculate_sentiment==True:\n",
    "        finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "        tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "        nlp = TextClassificationPipeline(model=finbert, tokenizer=tokenizer, device=0)  # Set device to 0 for GPU\n",
    "\n",
    "        # section_json_data_df = pd.read_excel(f'dataset_{section}.xlsx')\n",
    "\n",
    "        sentiments_list = []\n",
    "\n",
    "        for text in tqdm(df['BodyText']):\n",
    "            try:\n",
    "                sentiments_list.append(nlp(text[:1500])[0]['label'])\n",
    "            except:\n",
    "                sentiments_list.append(nlp(text[:1000])[0]['label'])\n",
    "\n",
    "        df['sentiments'] = sentiments_list\n",
    "\n",
    "        excel_file_path = f'{dataset_procesed_loc}\\\\dataset_{section}_with_sentiments.xlsx'\n",
    "        df = df.applymap(lambda x: x.encode('unicode_escape').\n",
    "                        decode('utf-8') if isinstance(x, str) else x)\n",
    "        df.to_excel(excel_file_path, index=False)\n",
    "    else:\n",
    "        df = pd.read_excel(f'{dataset_procesed_loc}\\\\dataset_{section}_with_sentiments.xlsx')\n",
    "        \n",
    "\n",
    "    df['MonthYear'] = df['PublicationDate_dates'].dt.to_period('M')\n",
    "    # Group by month and year, and calculate sentiments for each group\n",
    "    result = df.groupby(['MonthYear', 'sentiments']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    if 'Negative' not in result.columns:\n",
    "        result['Negative'] = 0*len(result)\n",
    "    if 'Positive' not in result.columns:\n",
    "        result['Positive'] = 0*len(result)\n",
    "    if 'Neutral' not in result.columns:\n",
    "        result['Neutral'] = 0*len(result)\n",
    "\n",
    "\n",
    "    result['MedianSentiment'] = result[['Negative', 'Neutral', 'Positive']].idxmax(axis=1)\n",
    "\n",
    "    # Calculate the total sentiments for each row\n",
    "    result['TotalSentiments'] = result[['Negative', 'Neutral', 'Positive']].sum(axis=1)\n",
    "\n",
    "    # Calculate the percentage of each sentiment for each row\n",
    "    result['PercentageNegative'] = (result['Negative'] / result['TotalSentiments']) * 100\n",
    "    result['PercentageNeutral'] = (result['Neutral'] / result['TotalSentiments']) * 100\n",
    "    result['PercentagePositive'] = (result['Positive'] / result['TotalSentiments']) * 100\n",
    "\n",
    "    # Drop the 'TotalSentiments' column if you don't need it in the final result\n",
    "    result.drop(columns=['TotalSentiments'], inplace=True)\n",
    "\n",
    "    result.to_csv(f'{dataset_procesed_loc}\\\\dataset_{section}_with_sentiments_groupby.csv', index=False)\n",
    "\n",
    "    return df   \n",
    "\n",
    "\n",
    "def get_word_cloud(section,df):\n",
    "\n",
    "    df['PublicationDate_dates'] = pd.to_datetime(df['PublicationDate_dates'], format='%Y-%m-%d', errors='coerce')\n",
    "    df['MonthYear'] = df['PublicationDate_dates'].dt.to_period('M')\n",
    "    df['BodyText'] = df['BodyText'].astype(str)\n",
    "    # grouped_df = df.groupby('MonthYear')['BodyText'].agg(lambda x: ' '.join((x))).reset_index()\n",
    "\n",
    "    import re \n",
    "    # Create and save individual word clouds for each year\n",
    "    for year in df['MonthYear'].unique():\n",
    "\n",
    "        text = ' '.join(df[df['MonthYear'] == year]['BodyText']).encode('utf-8').decode('unicode_escape', errors='ignore')\n",
    "\n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        \n",
    "        # Plot the WordCloud image\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Word Cloud for {year}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save the WordCloud image\n",
    "        plt.savefig(f'word_cloud/wordcloud_{year}.png')\n",
    "        plt.close()  # Close the plot to avoid displaying in the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pages: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6057cb4ac128499eb87bc175ea3d07dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles retrieved: 51\n",
      "****************************************************************************************************\n",
      "Working on business-to-business\n",
      "****************************************************************************************************\n",
      "Length of titles: 51\n",
      "Length of titles: 51\n",
      "Length of body_text: 51\n",
      "Length of publication_date: 51\n",
      "Length of production_office: 51\n",
      "Length of lang: 51\n",
      "Length of article_tags: 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rs659\\AppData\\Local\\Temp\\ipykernel_90868\\977182797.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.encode('unicode_escape').\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61923b4ffa7b47cf8de7690fafb8902c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rs659\\Desktop\\AIPI510proj\\projgitenv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "C:\\Users\\rs659\\AppData\\Local\\Temp\\ipykernel_90868\\977182797.py:213: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: x.encode('unicode_escape').\n"
     ]
    }
   ],
   "source": [
    "# sections_scrape = ['business-to-business', 'food', 'global-development', 'lifeandstyle', 'money', 'news', 'politics', 'society', 'world']\n",
    "# sections_scrape = ['business-to-business']#'business',,'money']\n",
    "sections_scrape = ['business','money']\n",
    "\n",
    "download_data(dataset_loc,sections_scrape)\n",
    "\n",
    "make_df(dataset_loc,sections_scrape,dataset_procesed_loc,calculate_sentiment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding data to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PercentageNegative</th>\n",
       "      <th>PercentageNeutral</th>\n",
       "      <th>PercentagePositive</th>\n",
       "      <th>Date</th>\n",
       "      <th>TotalSentiments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1991-12</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>24.180328</td>\n",
       "      <td>72.131148</td>\n",
       "      <td>3.688525</td>\n",
       "      <td>2022-09</td>\n",
       "      <td>488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>24.161074</td>\n",
       "      <td>67.337808</td>\n",
       "      <td>8.501119</td>\n",
       "      <td>2022-10</td>\n",
       "      <td>447.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>25.970874</td>\n",
       "      <td>67.475728</td>\n",
       "      <td>6.553398</td>\n",
       "      <td>2022-11</td>\n",
       "      <td>412.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>23.497268</td>\n",
       "      <td>68.852459</td>\n",
       "      <td>7.650273</td>\n",
       "      <td>2022-12</td>\n",
       "      <td>366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>20.992366</td>\n",
       "      <td>68.320611</td>\n",
       "      <td>10.687023</td>\n",
       "      <td>2023-01</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>374 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PercentageNegative  PercentageNeutral  PercentagePositive     Date  \\\n",
       "0            100.000000           0.000000            0.000000  1991-12   \n",
       "1              0.000000           0.000000            0.000000        0   \n",
       "2              0.000000           0.000000            0.000000        0   \n",
       "3              0.000000           0.000000            0.000000        0   \n",
       "4              0.000000           0.000000            0.000000        0   \n",
       "..                  ...                ...                 ...      ...   \n",
       "369           24.180328          72.131148            3.688525  2022-09   \n",
       "370           24.161074          67.337808            8.501119  2022-10   \n",
       "371           25.970874          67.475728            6.553398  2022-11   \n",
       "372           23.497268          68.852459            7.650273  2022-12   \n",
       "373           20.992366          68.320611           10.687023  2023-01   \n",
       "\n",
       "     TotalSentiments  \n",
       "0                1.0  \n",
       "1                0.0  \n",
       "2                0.0  \n",
       "3                0.0  \n",
       "4                0.0  \n",
       "..               ...  \n",
       "369            488.0  \n",
       "370            447.0  \n",
       "371            412.0  \n",
       "372            366.0  \n",
       "373            262.0  \n",
       "\n",
       "[374 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('../data/dataset_business_with_sentiments_groupby.csv')\n",
    "df = pd.read_csv('../data/news_sentiments_groupby_filled.csv')\n",
    "# df = pd.read_csv('../data/news_sentiments_groupby.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'], format=\"%Y-%m\")\n",
    "del df['MedianSentiment']\n",
    "del df['Negative']\n",
    "del df['Neutral']\n",
    "del df['Positive']\n",
    "del df['MonthYear']\n",
    "# del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PercentageNegative  PercentageNeutral  PercentagePositive  \\\n",
      "0               100.0                0.0                 0.0   \n",
      "1                 0.0                0.0                 0.0   \n",
      "2                 0.0                0.0                 0.0   \n",
      "3                 0.0                0.0                 0.0   \n",
      "4                 0.0                0.0                 0.0   \n",
      "\n",
      "                  Date  TotalSentiments  \n",
      "0  1991-12-01 00:00:00              1.0  \n",
      "1  1992-01-01 00:00:00              0.0  \n",
      "2  1992-02-01 00:00:00              0.0  \n",
      "3  1992-03-01 00:00:00              0.0  \n",
      "4  1992-04-01 00:00:00              0.0  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('../food_inflation_analysis.db')\n",
    "# save train data to the sqlite database as a new table named OCED_USA_FOOD_INFLATION\n",
    "df.to_sql('news_sentiments', conn, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "# query the database to get the train data\n",
    "query = '''SELECT * FROM news_sentiments'''\n",
    "new_sentiments_data = pd.read_sql(query, conn)\n",
    "# print head\n",
    "print(new_sentiments_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
