{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from transformers import TextClassificationPipeline\n",
    "import torch        \n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "sections_scrape = ['better-business', 'business-to-business', 'food', 'global-development', 'lifeandstyle', 'money', 'news', 'politics', 'society', 'world']\n",
    "dataset_loc = 'Dataset_RAW'\n",
    "dataset_procesed_loc = 'Dataset_preprocessed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_pages(url,params):\n",
    "    response = requests.get(url, params=params)\n",
    "    data = json.loads(response.text)\n",
    "    return data['response']['pages']\n",
    "\n",
    "\n",
    "def get_data(pages_to_scrape,section):\n",
    "        all_data = []\n",
    "\n",
    "        last_page_id = ''\n",
    "        for page in tqdm(range(1, pages_to_scrape + 1)):\n",
    "\n",
    "            if page==1:\n",
    "                url = \"https://content.guardianapis.com/search\"\n",
    "                params = {\n",
    "                    'show-tags': 'all',\n",
    "                    # 'show-fields': 'production-office,lang,sectionId,sectionName,webTitle,webUrl,headline,trailText,bodyText,firstPublicationDate,productionOffice',\n",
    "                    'show-fields': 'all',\n",
    "                    # 'page': 1,\n",
    "                    'page-size': 200,\n",
    "                    'api-key': 'test',\n",
    "                    'from-date': '1990-01-01',\n",
    "                    'to-date': '2023-01-20',\n",
    "                    'show-references': 'all',\n",
    "                    'use-date':'published',\n",
    "                    'section': str(section)\n",
    "                }\n",
    "            else:\n",
    "                url = f\"https://content.guardianapis.com/content/{last_page_id}/next?\"\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = json.loads(response.text)\n",
    "                articles = data['response']['results']\n",
    "                last_page_id = data['response']['results'][-1]['id']\n",
    "                all_data.extend(articles)\n",
    "            else:\n",
    "                print(f\"Error on page {page}: {response.status_code} - {response.text}\")\n",
    "\n",
    "        return all_data\n",
    "\n",
    "\n",
    "def download_data(dataset_loc,sections_scrape):\n",
    "\n",
    "    \n",
    "\n",
    "    for section in sections_scrape:\n",
    "\n",
    "        if os.path.isfile(f'{dataset_loc}\\\\dataset_{str(section)}.json'):\n",
    "            print(f'Raw Datset for section: {section} Exists.')\n",
    "            continue\n",
    "        \n",
    "        url = \"https://content.guardianapis.com/search\"\n",
    "        params = {\n",
    "            'show-tags': 'all',\n",
    "            # 'show-fields': 'production-office,lang,sectionId,sectionName,webTitle,webUrl,headline,trailText,bodyText,firstPublicationDate,productionOffice',\n",
    "            'show-fields': 'all',\n",
    "            'page': 1,\n",
    "            'page-size': 200,\n",
    "            'api-key': 'test',\n",
    "            'from-date': '1990-01-01',\n",
    "            'to-date': '2023-01-20',\n",
    "            'show-references': 'all',\n",
    "            'use-date':'published',\n",
    "            'section': str(section)\n",
    "        }\n",
    "\n",
    "\n",
    "        total_pages = get_total_pages(url,params)\n",
    "        print(f\"Total pages: {total_pages}\")\n",
    "\n",
    "        # Specify the number of pages you want to scrape\n",
    "        pages_to_scrape = total_pages\n",
    "\n",
    "        result_data = get_data(pages_to_scrape,section)\n",
    "        print(f\"Total articles retrieved: {len(result_data)}\")\n",
    "\n",
    "        dataset = {}\n",
    "\n",
    "        dataset['results'] = result_data\n",
    "\n",
    "        # Save the dictionary to a JSON file\n",
    "        with open(f'{dataset_loc}\\\\dataset_{str(section)}.json', 'w') as json_file:\n",
    "            json.dump(dataset, json_file)\n",
    "\n",
    "\n",
    "\n",
    "def make_df(dataset_loc,sections_scrape,dataset_procesed_loc,calculate_sentiment=True):\n",
    "\n",
    "    \n",
    "    for section in sections_scrape:\n",
    "        \n",
    "        print(\"*\"*100)\n",
    "        print(f'Working on {section}')\n",
    "        print(\"*\"*100)\n",
    "        # Assuming your JSON data is stored in a file named 'data.json'\n",
    "        with open(f'{dataset_loc}\\\\dataset_{str(section)}.json', 'r') as file:\n",
    "            result_data = json.load(file)\n",
    "        \n",
    "        small_data = result_data['results']\n",
    "\n",
    "        titles = [data['fields']['headline'] for data in small_data]\n",
    "        body_text = [data['fields']['bodyText'] for data in small_data]\n",
    "\n",
    "\n",
    "        publication_date = []\n",
    "        count= 0\n",
    "        for data in small_data:\n",
    "            if 'firstPublicationDate' in data['fields']:\n",
    "                publication_date.append(data['fields']['firstPublicationDate'])\n",
    "                count += 1\n",
    "            elif 'newspaperEditionDate' in data['fields']:\n",
    "                publication_date.append(data['fields']['newspaperEditionDate'])\n",
    "                count += 1\n",
    "            elif 'webPublicationDate' in data:\n",
    "                publication_date.append(data['webPublicationDate'])\n",
    "                count += 1\n",
    "            else:\n",
    "                print(json.dumps(data))\n",
    "                publication_date.append(None)  # or any default value if both keys are missing\n",
    "                break\n",
    "\n",
    "\n",
    "        production_office = []\n",
    "\n",
    "        for data in small_data:\n",
    "            if 'productionOffice' in data['fields']:\n",
    "                production_office.append(data['fields']['productionOffice'])\n",
    "            else:\n",
    "                production_office.append(None)  # or any default value if both keys are missing\n",
    "\n",
    "\n",
    "        lang = [data['fields']['lang'] for data in small_data]\n",
    "\n",
    "\n",
    "        article_tags = []\n",
    "        for articles in small_data:\n",
    "            # for article_data in articles['tags']:\n",
    "            #     # print([tags_data['id'] for tags_data in articles['tags']])\n",
    "            #     # article_tags.append([tags_data['id'] for tags_data in articles['tags']])\n",
    "            article_tags.append([tags_data['id'] for tags_data in articles['tags']])\n",
    "\n",
    "\n",
    "        # Get length of each variable\n",
    "        length_titles = len(titles)\n",
    "        length_body_text = len(body_text)\n",
    "        length_publication_date = len(publication_date)\n",
    "        length_production_office = len(production_office)\n",
    "        length_lang = len(lang)\n",
    "        length_article_tags = len(article_tags)\n",
    "\n",
    "        # Print the lengths\n",
    "        print(f'Length of titles: {len(small_data)}')\n",
    "        print(f'Length of titles: {length_titles}')\n",
    "        print(f'Length of body_text: {length_body_text}')\n",
    "        print(f'Length of publication_date: {length_publication_date}')\n",
    "        print(f'Length of production_office: {length_production_office}')\n",
    "        print(f'Length of lang: {length_lang}')\n",
    "        print(f'Length of article_tags: {length_article_tags}')\n",
    "\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'Title': titles,\n",
    "            'BodyText': body_text,\n",
    "            'PublicationDate': publication_date,\n",
    "            'ProductionOffice': production_office,\n",
    "            'Lang': lang,\n",
    "            'ArticleTags': article_tags\n",
    "        })\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        excel_file_path = f'{dataset_procesed_loc}\\\\dataset_{section}.xlsx'\n",
    "        df = df.applymap(lambda x: x.encode('unicode_escape').\n",
    "                         decode('utf-8') if isinstance(x, str) else x)\n",
    "        \n",
    "        df['PublicationDate'] = df['PublicationDate'].apply(lambda x: pd.to_datetime(x, format='%Y-%m-%dT%H:%M:%SZ'))\n",
    "        df['PublicationDate_dates'] = df['PublicationDate'].dt.date\n",
    "\n",
    "        df['PublicationDate_dates'] = pd.to_datetime(df['PublicationDate_dates'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "        df.to_excel(f'{dataset_procesed_loc}\\\\dataset_{section}_WIHTOUT_Sentiments.xlsx', index=False)\n",
    "        \n",
    "        df = get_sentiments(df,section,dataset_procesed_loc,calculate_sentiment)\n",
    "\n",
    "        # get_word_cloud(section,df)\n",
    "\n",
    "        # df.to_excel(excel_file_path, index=False)\n",
    " \n",
    "\n",
    "def get_sentiments(df,section,dataset_procesed_loc,calculate_sentiment=True):\n",
    "\n",
    "    \n",
    "    if calculate_sentiment==True:\n",
    "        finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "        tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "\n",
    "        nlp = TextClassificationPipeline(model=finbert, tokenizer=tokenizer, device=0)  # Set device to 0 for GPU\n",
    "\n",
    "        # section_json_data_df = pd.read_excel(f'dataset_{section}.xlsx')\n",
    "\n",
    "        sentiments_list = []\n",
    "\n",
    "        for text in tqdm(df['BodyText']):\n",
    "            try:\n",
    "                sentiments_list.append(nlp(text[:1500])[0]['label'])\n",
    "            except:\n",
    "                sentiments_list.append(nlp(text[:1000])[0]['label'])\n",
    "\n",
    "        df['sentiments'] = sentiments_list\n",
    "\n",
    "        excel_file_path = f'{dataset_procesed_loc}\\\\dataset_{section}_with_sentiments.xlsx'\n",
    "        df = df.applymap(lambda x: x.encode('unicode_escape').\n",
    "                        decode('utf-8') if isinstance(x, str) else x)\n",
    "        df.to_excel(excel_file_path, index=False)\n",
    "    else:\n",
    "        df = pd.read_excel(f'{dataset_procesed_loc}\\\\dataset_{section}_with_sentiments.xlsx')\n",
    "        \n",
    "\n",
    "    df['MonthYear'] = df['PublicationDate_dates'].dt.to_period('M')\n",
    "    # Group by month and year, and calculate sentiments for each group\n",
    "    result = df.groupby(['MonthYear', 'sentiments']).size().unstack(fill_value=0).reset_index()\n",
    "\n",
    "    if 'Negative' not in result.columns:\n",
    "        result['Negative'] = 0*len(result)\n",
    "    if 'Positive' not in result.columns:\n",
    "        result['Positive'] = 0*len(result)\n",
    "    if 'Neutral' not in result.columns:\n",
    "        result['Neutral'] = 0*len(result)\n",
    "\n",
    "\n",
    "    result['MedianSentiment'] = result[['Negative', 'Neutral', 'Positive']].idxmax(axis=1)\n",
    "\n",
    "    # Calculate the total sentiments for each row\n",
    "    result['TotalSentiments'] = result[['Negative', 'Neutral', 'Positive']].sum(axis=1)\n",
    "\n",
    "    # Calculate the percentage of each sentiment for each row\n",
    "    result['PercentageNegative'] = (result['Negative'] / result['TotalSentiments']) * 100\n",
    "    result['PercentageNeutral'] = (result['Neutral'] / result['TotalSentiments']) * 100\n",
    "    result['PercentagePositive'] = (result['Positive'] / result['TotalSentiments']) * 100\n",
    "\n",
    "    # Drop the 'TotalSentiments' column if you don't need it in the final result\n",
    "    result.drop(columns=['TotalSentiments'], inplace=True)\n",
    "\n",
    "    result.to_csv(f'{dataset_procesed_loc}\\\\dataset_{section}_with_sentiments_groupby.csv', index=False)\n",
    "\n",
    "    return df   \n",
    "\n",
    "\n",
    "def get_word_cloud(section,df):\n",
    "\n",
    "    df['PublicationDate_dates'] = pd.to_datetime(df['PublicationDate_dates'], format='%Y-%m-%d', errors='coerce')\n",
    "    df['MonthYear'] = df['PublicationDate_dates'].dt.to_period('M')\n",
    "    df['BodyText'] = df['BodyText'].astype(str)\n",
    "    # grouped_df = df.groupby('MonthYear')['BodyText'].agg(lambda x: ' '.join((x))).reset_index()\n",
    "\n",
    "    import re \n",
    "    # Create and save individual word clouds for each year\n",
    "    for year in df['MonthYear'].unique():\n",
    "\n",
    "        text = ' '.join(df[df['MonthYear'] == year]['BodyText']).encode('utf-8').decode('unicode_escape', errors='ignore')\n",
    "\n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "        \n",
    "        # Plot the WordCloud image\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(f'Word Cloud for {year}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Save the WordCloud image\n",
    "        plt.savefig(f'word_cloud/wordcloud_{year}.png')\n",
    "        plt.close()  # Close the plot to avoid displaying in the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections_scrape = ['business-to-business', 'food', 'global-development', 'lifeandstyle', 'money', 'news', 'politics', 'society', 'world']\n",
    "\n",
    "download_data(dataset_loc,sections_scrape)\n",
    "\n",
    "make_df(dataset_loc,sections_scrape,dataset_procesed_loc,calculate_sentiment=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding data to SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/rishabhshah/Desktop/aipi510proj/food_inflation_analysis/data/dataset_business_with_sentiments_groupby.csv')\n",
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MonthYear  Negative  Neutral  Positive MedianSentiment\n",
      "0   1991-12         1        0         0        Negative\n",
      "1   1992-09         0        1         0         Neutral\n",
      "2   1994-11         1        0         0        Negative\n",
      "3   1996-02         1        0         0        Negative\n",
      "4   1997-02         1        2         1         Neutral\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('../food_inflation_analysis.db')\n",
    "# save train data to the sqlite database as a new table named OCED_USA_FOOD_INFLATION\n",
    "df.to_sql('news_sentiments', conn, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "# query the database to get the train data\n",
    "query = '''SELECT * FROM news_sentiments'''\n",
    "new_sentiments_data = pd.read_sql(query, conn)\n",
    "# print head\n",
    "print(new_sentiments_data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
